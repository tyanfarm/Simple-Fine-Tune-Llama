{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl=0.19.1 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install latest transformers for Gemma 3N\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95555ac1",
   "metadata": {},
   "source": [
    "## Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9922024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9f182",
   "metadata": {},
   "source": [
    "## Gemma3n Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723eb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    _ = model.generate(\n",
    "        **tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\"),\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{ \"type\" : \"text\",\n",
    "                  \"text\" : \"Write a poem about sloths.\" }]\n",
    "}]\n",
    "do_gemma_3n_inference(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a18ab",
   "metadata": {},
   "source": [
    "## Finetuning Gemma3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632fc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac8be6",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439fd26",
   "metadata": {},
   "source": [
    "We now use the `Gemma-3` format for conversation style finetunes. We use [hotel_conversations_118](https://huggingface.co/datasets/tyanfarm/hotel_conversations_118) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d255f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027a6bf",
   "metadata": {},
   "source": [
    "#### Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20538176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tyanfarm/hotel_conversations_118\", split = \"train\")\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3cbde4",
   "metadata": {},
   "source": [
    "#### Standard Formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45155a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)\n",
    "\n",
    "dataset[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ae89d",
   "metadata": {},
   "source": [
    "#### Gemma-3 Formatted dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d95bb8",
   "metadata": {},
   "source": [
    "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f57526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples, indices):\n",
    "    convos = examples[\"conversations\"]\n",
    "    results = []\n",
    "    for i, convo in zip(indices, convos):\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix('<bos>')\n",
    "            results.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at index {i}: {e}\")\n",
    "            print(f\"Hotel data has error: {str(convo)}\")\n",
    "            results.append(\"\")  # Hoặc raise nếu muốn dừng luôn\n",
    "    return { \"text\": results }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1509257",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb873a",
   "metadata": {},
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908870ec",
   "metadata": {},
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4d4d7",
   "metadata": {},
   "source": [
    "Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3ede4",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80c6c7",
   "metadata": {},
   "source": [
    "Show final memory & time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1562912",
   "metadata": {},
   "source": [
    "## Push to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0144b3",
   "metadata": {},
   "source": [
    "#### LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset_name = \"tyanfarm/gemma-3n-hotels-conversational-118-finetuned\"\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=dataset_name, repo_type=\"dataset\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3n\")\n",
    "tokenizer.save_pretrained(\"gemma-3n\")\n",
    "\n",
    "# Push to huggingface\n",
    "model.push_to_hub(dataset_name)\n",
    "tokenizer.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be78435",
   "metadata": {},
   "source": [
    "#### Merged Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33baa4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from unsloth import FastModel\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"tyanfarm/gemma-3n-hotels-conversational-118-finetuned\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3N?\",}]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"tyanfarm/gemma-3N-finetune\", tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77babf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3N-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1e782",
   "metadata": {},
   "source": [
    "#### GGUF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Tải toàn bộ kho lưu trữ mô hình từ Hugging Face Hub\n",
    "snapshot_download(\n",
    "    repo_id=\"tyanfarm/gemma-3N-finetune\",  # Thay bằng tên mô hình trên Hub\n",
    "    local_dir=\"gemma-3N-finetune-gguf\",  # Thư mục cục bộ để lưu mô hình\n",
    "    token=\"\"  # Token Hugging Face nếu mô hình là private\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.create_repo(repo_id=\"tyanfarm/gemma-3n-hotels-conversational-118-finetuned-gguf\", repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950eca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3N-finetune\",\n",
    "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef967b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\n",
    "    \"gemma-3N-finetune-gguf\",\n",
    "    quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "    repo_id = \"tyanfarm/gemma-3n-hotels-conversational-118-finetuned-gguf\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
