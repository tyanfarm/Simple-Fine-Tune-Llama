{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5099ea",
   "metadata": {},
   "source": [
    "# 1. Packages Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba8703",
   "metadata": {},
   "source": [
    "- `--no-deps`: Kh√¥ng c√†i ƒë·∫∑t t·ª± ƒë·ªông c√°c ph·ª• thu·ªôc c·ªßa c√°c g√≥i n√†y, tr√°nh xung ƒë·ªôt phi√™n b·∫£n trong Colab (v√¨ Colab ƒë√£ c√≥ m·ªôt s·ªë th∆∞ vi·ªán c√†i s·∫µn).\n",
    "\n",
    "- `bitsandbytes`: Th∆∞ vi·ªán h·ªó tr·ª£ l∆∞·ª£ng t·ª≠ h√≥a (quantization) nh∆∞ 4-bit, 8-bit, gi√∫p gi·∫£m dung l∆∞·ª£ng b·ªô nh·ªõ khi ch·∫°y m√¥ h√¨nh l·ªõn.\n",
    "\n",
    "- `accelerate`: Th∆∞ vi·ªán c·ªßa Hugging Face ƒë·ªÉ tƒÉng t·ªëc hu·∫•n luy·ªán v√† inference tr√™n nhi·ªÅu thi·∫øt b·ªã (CPU, GPU, TPU).\n",
    "\n",
    "- `xformers==0.0.29`: Th∆∞ vi·ªán t·ªëi ∆∞u h√≥a attention trong Transformer, c·∫£i thi·ªán t·ªëc ƒë·ªô v√† gi·∫£m b·ªô nh·ªõ. Ch·ªâ ƒë·ªãnh phi√™n b·∫£n 0.0.29 ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch.\n",
    "\n",
    "- `peft`: Th∆∞ vi·ªán Parameter-Efficient Fine-Tuning c·ªßa Hugging Face, h·ªó tr·ª£ c√°c k·ªπ thu·∫≠t nh∆∞ LoRA/QLoRA m√† Unsloth s·ª≠ d·ª•ng.\n",
    "\n",
    "- `trl`: Th∆∞ vi·ªán Transformers Reinforcement Learning, h·ªó tr·ª£ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi c√°c ph∆∞∆°ng ph√°p nh∆∞ RLHF (Reinforcement Learning from Human Feedback).\n",
    "\n",
    "- `triton`: Th∆∞ vi·ªán t·ª´ OpenAI ƒë·ªÉ t·ªëi ∆∞u h√≥a kernel GPU, tƒÉng t·ªëc t√≠nh to√°n trong PyTorch.\n",
    "\n",
    "- `cut_cross_entropy`: M·ªôt g√≥i t·ªëi ∆∞u h√≥a h√†m m·∫•t m√°t cross-entropy, th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ tƒÉng t·ªëc hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ.\n",
    "\n",
    "- `unsloth_zoo`: M·ªôt g√≥i ph·ª• c·ªßa Unsloth, cung c·∫•p c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ho·∫∑c c√°c c√¥ng c·ª• b·ªï sung ƒë·ªÉ l√†m vi·ªác v·ªõi Unsloth.\n",
    "\n",
    "- `sentencepiece`: Th∆∞ vi·ªán m√£ h√≥a vƒÉn b·∫£n (tokenization), th∆∞·ªùng d√πng cho c√°c m√¥ h√¨nh nh∆∞ Llama ho·∫∑c DeepSeek.\n",
    "\n",
    "- `protobuf`: Th∆∞ vi·ªán Google Protocol Buffers, c·∫ßn thi·∫øt ƒë·ªÉ l√†m vi·ªác v·ªõi ƒë·ªãnh d·∫°ng d·ªØ li·ªáu trong m·ªôt s·ªë m√¥ h√¨nh ho·∫∑c c√¥ng c·ª• Hugging Face.\n",
    "\n",
    "- `datasets`: Th∆∞ vi·ªán c·ªßa Hugging Face ƒë·ªÉ t·∫£i v√† x·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán/inference.\n",
    "\n",
    "- `huggingface_hub`: Th∆∞ vi·ªán ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Hugging Face Hub (t·∫£i m√¥ h√¨nh, dataset, ƒë·∫©y k·∫øt qu·∫£ l√™n Hub).\n",
    "\n",
    "- `hf_transfer`: C√¥ng c·ª• tƒÉng t·ªëc t·∫£i xu·ªëng t·ª´ Hugging Face Hub, h·ªØu √≠ch khi t·∫£i c√°c m√¥ h√¨nh l·ªõn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# import os\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     !pip install unsloth\n",
    "# else:\n",
    "#     # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
    "#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "#     !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "#     !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b178e0",
   "metadata": {},
   "source": [
    "# 2. Config LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b3c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5080\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"12.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b622c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyan/Simple-Fine-Tune-Llama/finetune_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5080. Num GPUs = 1. Max memory: 15.92 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e91a7",
   "metadata": {},
   "source": [
    "# 3. LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d5c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT)\n",
    "# LoRA (Low-Rank Adaptation)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,    # pre-trained model\n",
    "    r = 32,\n",
    "    # q_proj, k_proj, v_proj: C√°c l·ªõp t·∫°o Query, Key, Value trong c∆° ch·∫ø Attention.\n",
    "    # o_proj: L·ªõp chi·∫øu ƒë·∫ßu ra c·ªßa Attention.\n",
    "    # gate_proj, up_proj, down_proj: C√°c l·ªõp trong m·∫°ng Feed-Forward (FFN)\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # C√°c ma tr·∫≠n Low-Rank (A.B) ƒë∆∞·ª£c nh√¢n v·ªõi lora_alpha/r ƒë·ªÉ ƒëi·ªÅu ch·ªânh m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa ch√∫ng l√™n tr·ªçng s·ªë g·ªëc.\n",
    "    lora_alpha = 32,\n",
    "\n",
    "    # Dropout l√† k·ªπ thu·∫≠t regularization, ng·∫´u nhi√™n b·ªè qua m·ªôt t·ª∑ l·ªá ƒë∆°n v·ªã ƒë·ªÉ tr√°nh overfitting.\n",
    "    lora_dropout = 0,\n",
    "\n",
    "    # Quy ƒë·ªãnh c√°ch x·ª≠ l√Ω bias (ƒë·ªô l·ªách) trong c√°c l·ªõp LoRA.\n",
    "    bias = \"none\",\n",
    "\n",
    "    # K√≠ch ho·∫°t gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ khi hu·∫•n luy·ªán.\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "\n",
    "    # RS-LoRA l√† m·ªôt bi·∫øn th·ªÉ c·ªßa LoRA, c·∫£i thi·ªán t√≠nh ·ªïn ƒë·ªãnh khi d√πng rank cao.\n",
    "    # N·∫øu b·∫°n tƒÉng r v√† g·∫∑p v·∫•n ƒë·ªÅ ·ªïn ƒë·ªãnh, c√≥ th·ªÉ th·ª≠ b·∫≠t True.\n",
    "    use_rslora = False,\n",
    "\n",
    "    # LoFTQ k·∫øt h·ª£p quantization v·ªõi LoRA ƒë·ªÉ n√©n m√¥ h√¨nh th√™m.\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9126f9",
   "metadata": {},
   "source": [
    "# 4. Format Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f55356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:            \n",
    "# Company database: {}\n",
    "\n",
    "# ### Input:\n",
    "# SQL Prompt: {}\n",
    "\n",
    "# ### Response:\n",
    "# SQL: {}\n",
    "\n",
    "# Explanation: {}\n",
    "# \"\"\"\n",
    "\n",
    "# # End Of Sequence Token\n",
    "# EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#   company_databases = examples[\"sql_context\"]\n",
    "#   prompts = examples[\"sql_prompt\"]\n",
    "#   sqls = examples[\"sql\"]\n",
    "#   explanations = examples[\"sql_explanation\"]\n",
    "#   texts = []\n",
    "\n",
    "#   for company_database, prompt, sql, explanation in zip(company_databases, prompts, sqls, explanations):\n",
    "#     # Must add EOS_TOKEN, otherwise your generation wil go on forever!\n",
    "#     # .format replace `{}`\n",
    "#     text = alpaca_prompt.format(company_database, prompt, sql, explanation) + EOS_TOKEN\n",
    "#     texts.append(text)\n",
    "\n",
    "#   return {\"text\" : texts, }\n",
    "\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "login(token=\"\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e846b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Of Sequence Token (r·∫•t quan tr·ªçng)\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_qa_func(examples):\n",
    "    all_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        text += EOS_TOKEN\n",
    "        all_texts.append(text)\n",
    "        \n",
    "    return {\"text\": all_texts}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717da6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'T√™n c·ªßa kh√°ch s·∫°n l√† g√¨?'},\n",
       "  {'role': 'assistant', 'content': 'T√™n kh√°ch s·∫°n l√† Simon Hotel.'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 22 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nT√™n c·ªßa kh√°ch s·∫°n l√† g√¨?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nT√™n kh√°ch s·∫°n l√† Simon Hotel.<|eot_id|><|eot_id|>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/data.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_qa_func, batched=True)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea5f31",
   "metadata": {},
   "source": [
    "# 5. SFTTrainer (Supervised Fine-Tuning Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeabdbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset=None,\n",
    "    max_seq_length = max_seq_length,\n",
    "    # S·ªë l∆∞·ª£ng ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu.\n",
    "    dataset_num_proc = 1,\n",
    "\n",
    "    # N·∫øu True, c√°c m·∫´u ng·∫Øn s·∫Ω ƒë∆∞·ª£c g·ªôp l·∫°i ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa max_seq_length, tƒÉng hi·ªáu qu·∫£ hu·∫•n luy·ªán.\n",
    "    packing = False,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        # K√≠ch th∆∞·ªõc batch tr√™n m·ªói thi·∫øt b·ªã (GPU/CPU).\n",
    "        per_device_train_batch_size = 4,\n",
    "\n",
    "        # S·ªë b∆∞·ªõc t√≠ch l≈©y gradient tr∆∞·ªõc khi c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n",
    "        gradient_accumulation_steps = 4,\n",
    "\n",
    "        # S·ªë b∆∞·ªõc kh·ªüi ƒë·ªông ƒë·ªÉ tƒÉng d·∫ßn learning rate.\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1,     # Set this for full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "\n",
    "        # N·∫øu kh√¥ng h·ªó tr·ª£ BF16 (16-bit floating point), FP16 ƒë∆∞·ª£c d√πng ƒë·ªÉ gi·∫£m b·ªô nh·ªõ v√† tƒÉng t·ªëc, nh∆∞ng c√≥ th·ªÉ gi·∫£m ƒë·ªô ch√≠nh x√°c m·ªôt ch√∫t.\n",
    "        #fp16 = is_bfloat16_supported(),\n",
    "\n",
    "        # BF16 gi·ªØ ph·∫°m vi s·ªë l·ªõn h∆°n FP16, t·ªët h∆°n cho hu·∫•n luy·ªán m√¥ h√¨nh l·ªõn. N·∫øu ph·∫ßn c·ª©ng h·ªó tr·ª£ (nh∆∞ GPU NVIDIA Ampere tr·ªü l√™n), BF16 ƒë∆∞·ª£c ∆∞u ti√™n.\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "\n",
    "        # B·ªô t·ªëi ∆∞u h√≥a (optimizer) ƒë∆∞·ª£c s·ª≠ d·ª•ng.\n",
    "        optim = \"adamw_8bit\",\n",
    "\n",
    "        # H·ªá s·ªë ƒëi·ªÅu ch·ªânh suy gi·∫£m tr·ªçng s·ªë (regularization).\n",
    "        weight_decay = 0.01,\n",
    "\n",
    "        # Learning rate gi·∫£m d·∫ßn tuy·∫øn t√≠nh t·ª´ gi√° tr·ªã t·ªëi ƒëa (2e-4) v·ªÅ 0 qua 60 b∆∞·ªõc, gi√∫p m√¥ h√¨nh h·ªôi t·ª• t·ªët h∆°n v√†o cu·ªëi.\n",
    "        lr_scheduler_type = \"linear\",\n",
    "\n",
    "        # random seed\n",
    "        seed = 3047,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b414806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 37 | Num Epochs = 20 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 22,544,384 of 1,258,358,784 (1.79% trained)\n",
      "CUDA error (/__w/xformers/xformers/third_party/flash-attention/hopper/flash_bwd_launch_template.h:219): invalid argument\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\"llama3-sql\", tokenizer, quantization_method=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe378282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r /content/llama3-sql.zip /content/llama3-sql"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
