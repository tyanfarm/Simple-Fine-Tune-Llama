{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-07T09:26:26.448032Z",
     "iopub.status.busy": "2025-10-07T09:26:26.447421Z",
     "iopub.status.idle": "2025-10-07T09:26:26.758732Z",
     "shell.execute_reply": "2025-10-07T09:26:26.757994Z",
     "shell.execute_reply.started": "2025-10-07T09:26:26.447997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:26:26.760500Z",
     "iopub.status.busy": "2025-10-07T09:26:26.760116Z",
     "iopub.status.idle": "2025-10-07T09:26:59.791646Z",
     "shell.execute_reply": "2025-10-07T09:26:59.790578Z",
     "shell.execute_reply.started": "2025-10-07T09:26:26.760474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:26:59.793518Z",
     "iopub.status.busy": "2025-10-07T09:26:59.792886Z",
     "iopub.status.idle": "2025-10-07T09:26:59.877362Z",
     "shell.execute_reply": "2025-10-07T09:26:59.876791Z",
     "shell.execute_reply.started": "2025-10-07T09:26:59.793495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:26:59.879397Z",
     "iopub.status.busy": "2025-10-07T09:26:59.879136Z",
     "iopub.status.idle": "2025-10-07T09:28:10.977504Z",
     "shell.execute_reply": "2025-10-07T09:28:10.976890Z",
     "shell.execute_reply.started": "2025-10-07T09:26:59.879381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:10.978467Z",
     "iopub.status.busy": "2025-10-07T09:28:10.978255Z",
     "iopub.status.idle": "2025-10-07T09:28:17.706814Z",
     "shell.execute_reply": "2025-10-07T09:28:17.706228Z",
     "shell.execute_reply.started": "2025-10-07T09:28:10.978450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT)\n",
    "# LoRA (Low-Rank Adaptation)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,    # pre-trained model\n",
    "    r = 16,\n",
    "    # q_proj, k_proj, v_proj: Các lớp tạo Query, Key, Value trong cơ chế Attention.\n",
    "    # o_proj: Lớp chiếu đầu ra của Attention.\n",
    "    # gate_proj, up_proj, down_proj: Các lớp trong mạng Feed-Forward (FFN)\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # Các ma trận Low-Rank (A.B) được nhân với lora_alpha/r để điều chỉnh mức độ ảnh hưởng của chúng lên trọng số gốc.\n",
    "    lora_alpha = 32,\n",
    "\n",
    "    # Dropout là kỹ thuật regularization, ngẫu nhiên bỏ qua một tỷ lệ đơn vị để tránh overfitting.\n",
    "    lora_dropout = 0,\n",
    "\n",
    "    # Quy định cách xử lý bias (độ lệch) trong các lớp LoRA.\n",
    "    bias = \"none\",\n",
    "\n",
    "    # Kích hoạt gradient checkpointing để tiết kiệm bộ nhớ khi huấn luyện.\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "\n",
    "    # RS-LoRA là một biến thể của LoRA, cải thiện tính ổn định khi dùng rank cao.\n",
    "    # Nếu bạn tăng r và gặp vấn đề ổn định, có thể thử bật True.\n",
    "    use_rslora = False,\n",
    "\n",
    "    # LoFTQ kết hợp quantization với LoRA để nén mô hình thêm.\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:17.707754Z",
     "iopub.status.busy": "2025-10-07T09:28:17.707533Z",
     "iopub.status.idle": "2025-10-07T09:28:18.216810Z",
     "shell.execute_reply": "2025-10-07T09:28:18.215424Z",
     "shell.execute_reply.started": "2025-10-07T09:28:17.707737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chat_prompt = \"\"\"### Instruction\n",
    "{}\n",
    "\n",
    "### Input\n",
    "{}\n",
    "\n",
    "### Response\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:18.218099Z",
     "iopub.status.busy": "2025-10-07T09:28:18.217746Z",
     "iopub.status.idle": "2025-10-07T09:28:19.119523Z",
     "shell.execute_reply": "2025-10-07T09:28:19.118705Z",
     "shell.execute_reply.started": "2025-10-07T09:28:18.218070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token    # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instruction = \"\"\n",
    "    inputs = examples[\"question\"]\n",
    "    outputs = examples[\"answer\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = chat_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\": texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:19.120643Z",
     "iopub.status.busy": "2025-10-07T09:28:19.120373Z",
     "iopub.status.idle": "2025-10-07T09:28:22.938030Z",
     "shell.execute_reply": "2025-10-07T09:28:22.937317Z",
     "shell.execute_reply.started": "2025-10-07T09:28:19.120619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset(\"tyanfarm/hotels-questions-answers-mixed\", split = \"train\")\n",
    "# dataset_val = load_dataset(\"tyanfarm/hotels-questions-answers-mixed\", split = \"validation\")\n",
    "dataset_train = dataset_train.map(formatting_prompts_func, batched = True,)\n",
    "# dataset_val = dataset_val.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:22.939200Z",
     "iopub.status.busy": "2025-10-07T09:28:22.938711Z",
     "iopub.status.idle": "2025-10-07T09:28:22.942107Z",
     "shell.execute_reply": "2025-10-07T09:28:22.941407Z",
     "shell.execute_reply.started": "2025-10-07T09:28:22.939181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset[len(dataset)-5:len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:22.944430Z",
     "iopub.status.busy": "2025-10-07T09:28:22.944233Z",
     "iopub.status.idle": "2025-10-07T09:28:29.030327Z",
     "shell.execute_reply": "2025-10-07T09:28:29.029582Z",
     "shell.execute_reply.started": "2025-10-07T09:28:22.944416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    # eval_dataset = dataset_val,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #max_steps = 120,\n",
    "        num_train_epochs = 4,\n",
    "        learning_rate = 2e-4,\n",
    "        #fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:29.031753Z",
     "iopub.status.busy": "2025-10-07T09:28:29.031449Z",
     "iopub.status.idle": "2025-10-07T09:28:29.506183Z",
     "shell.execute_reply": "2025-10-07T09:28:29.505501Z",
     "shell.execute_reply.started": "2025-10-07T09:28:29.031722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"### Instruction\\n\\n\\n### Input\\n\",\n",
    "    response_part = \"### Response\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:29.507414Z",
     "iopub.status.busy": "2025-10-07T09:28:29.507165Z",
     "iopub.status.idle": "2025-10-07T09:28:35.437231Z",
     "shell.execute_reply": "2025-10-07T09:28:35.436592Z",
     "shell.execute_reply.started": "2025-10-07T09:28:29.507379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-07T09:28:35.438721Z",
     "iopub.status.busy": "2025-10-07T09:28:35.438084Z",
     "iopub.status.idle": "2025-10-07T12:24:17.173154Z",
     "shell.execute_reply": "2025-10-07T12:24:17.172501Z",
     "shell.execute_reply.started": "2025-10-07T09:28:35.438702Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:17.174506Z",
     "iopub.status.busy": "2025-10-07T12:24:17.174166Z",
     "iopub.status.idle": "2025-10-07T12:24:20.115017Z",
     "shell.execute_reply": "2025-10-07T12:24:20.114390Z",
     "shell.execute_reply.started": "2025-10-07T12:24:17.174487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # For faster Inference\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    chat_prompt.format(\n",
    "        \"\", # instruction - leave this blank!\n",
    "        \"Norfolk Saigon Hotel có phòng Superior không?\", # input\n",
    "        \"\", # output - leave this blank!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=512, \n",
    "    temperature=0.1,\n",
    "    top_k=10,\n",
    "    top_p=0.95)\n",
    "decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:20.115965Z",
     "iopub.status.busy": "2025-10-07T12:24:20.115709Z",
     "iopub.status.idle": "2025-10-07T12:24:20.228655Z",
     "shell.execute_reply": "2025-10-07T12:24:20.228125Z",
     "shell.execute_reply.started": "2025-10-07T12:24:20.115925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:20.229566Z",
     "iopub.status.busy": "2025-10-07T12:24:20.229334Z",
     "iopub.status.idle": "2025-10-07T12:24:20.351109Z",
     "shell.execute_reply": "2025-10-07T12:24:20.350476Z",
     "shell.execute_reply.started": "2025-10-07T12:24:20.229544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "model_name = \"tyanfarm/llama3-8b-hotels-information-mixed-finetuned\"\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=model_name, repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:20.352416Z",
     "iopub.status.busy": "2025-10-07T12:24:20.352152Z",
     "iopub.status.idle": "2025-10-07T12:24:31.814538Z",
     "shell.execute_reply": "2025-10-07T12:24:31.813923Z",
     "shell.execute_reply.started": "2025-10-07T12:24:20.352393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:31.815521Z",
     "iopub.status.busy": "2025-10-07T12:24:31.815274Z",
     "iopub.status.idle": "2025-10-07T12:24:35.780938Z",
     "shell.execute_reply": "2025-10-07T12:24:35.780272Z",
     "shell.execute_reply.started": "2025-10-07T12:24:31.815497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install flask python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:24:35.782480Z",
     "iopub.status.busy": "2025-10-07T12:24:35.782171Z",
     "iopub.status.idle": "2025-10-07T12:24:36.210518Z",
     "shell.execute_reply": "2025-10-07T12:24:36.209897Z",
     "shell.execute_reply.started": "2025-10-07T12:24:35.782444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "TOKEN = \"\"\n",
    "CHAT_ID = \"\"\n",
    "\n",
    "\n",
    "def send_message(text: str):\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage\"\n",
    "    payload = {\n",
    "        \"chat_id\": CHAT_ID,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"MarkdownV2\"  # optional: 'MarkdownV2' or 'HTML'\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "text = \"\"\"\n",
    "Training done \\!\n",
    "```kaggle-finetuning\n",
    "tyanfarm/llama3-8b-hotels-information-mixed-finetuned\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "send_message(text)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
