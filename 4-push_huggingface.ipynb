{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39b592f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #b1d1ff; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "Push HuggingFace\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de8eeb",
   "metadata": {},
   "source": [
    "## Push to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea721b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60375140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tyan\\Quickom\\Simple-Fine-Tune-Llama\\finetune_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "list_conversations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a24504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_addresses.json with 1000 conversations.\n",
      "Total conversations loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-hotel_addresses.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ab446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_roomtypes.json with 1000 conversations.\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# file_path = \"./finetuned-datasets/5-hotel_roomtypes.json\"\n",
    "# normalized = []\n",
    "\n",
    "# if file_path.endswith(\".json\"):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         data = json.load(file)\n",
    "#         print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "#         if isinstance(data, list):\n",
    "#             list_conversations.extend(data)\n",
    "\n",
    "# for conv in list_conversations:\n",
    "#     answer = conv.get(\"answer\", \"\")\n",
    "#     match = re.search(r\"(\\d+\\s*[–-]?\\s*\\d*)\\s*m²\\.\", answer)\n",
    "#     if match:\n",
    "#         value = match.group(1).replace(\" \", \"\")\n",
    "#         print(answer)\n",
    "#         conv[\"answer\"] = f\"Rộng khoảng {value} m².\"\n",
    "#         print(f\"{conv[\"answer\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51dc1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_emails.json with 300 conversations.\n",
      "Total conversations loaded: 1300\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-hotel_emails.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a276821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_phones.json with 300 conversations.\n",
      "Total conversations loaded: 1600\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-hotel_phones.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d3d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_roomtypes.json with 1000 conversations.\n",
      "Total conversations loaded: 2600\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-hotel_roomtypes.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc5659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-hotel_general.json with 1000 conversations.\n",
      "Total conversations loaded: 3600\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-hotel_general.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3a9f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./finetuned-datasets/5-baseline_general.json with 200 conversations.\n",
      "Total conversations loaded: 3800\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./finetuned-datasets/5-baseline_general.json\"\n",
    "\n",
    "if file_path.endswith(\".json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Loaded {file_path} with {len(data)} conversations.\")\n",
    "        if isinstance(data, list):\n",
    "            list_conversations.extend(data)\n",
    "\n",
    "print(f\"Total conversations loaded: {len(list_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524e5f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Bánh cuốn Thanh Trì làm bằng gì?',\n",
       "  'answer': 'Bánh từ bột gạo hấp mỏng, không nhân, cuộn lại ăn kèm chả lụa và nước mắm chua ngọt đặc trưng.'},\n",
       " {'question': 'Tranh Đông Hồ vẽ về gì?',\n",
       "  'answer': 'Tranh dân gian với đề tài sinh hoạt, ước vọng dân gian như gia đình, lễ Tết, cảnh vật quê hương.'},\n",
       " {'question': 'Quốc hoa của Việt Nam là gì?',\n",
       "  'answer': 'Dù chưa chính thức công nhận, hoa sen thường được coi là biểu tượng quốc hoa của Việt Nam.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_conversations[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc23c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8661d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, concatenate_datasets, interleave_datasets, DatasetDict\n",
    "# import math, random\n",
    "\n",
    "# hotel_files = [\n",
    "#     \"finetuned-datasets/5-hotel_addresses.json\",\n",
    "#     \"finetuned-datasets/5-hotel_emails.json\",\n",
    "#     \"finetuned-datasets/5-hotel_general.json\",\n",
    "#     \"finetuned-datasets/5-hotel_phones.json\",\n",
    "#     \"finetuned-datasets/5-hotel_roomtypes.json\",\n",
    "# ]\n",
    "\n",
    "# baseline_file = \"finetuned-datasets/5-baseline_general.json\"\n",
    "\n",
    "# hotel_sets = [load_dataset(\"json\", data_files=file, split=\"train\") for file in hotel_files]\n",
    "# hotel = concatenate_datasets(hotel_sets).shuffle(seed=42).add_column(\"topic\", [\"hotel\"] * sum(len(ds) for ds in hotel_sets))\n",
    "\n",
    "# baseline = load_dataset(\"json\", data_files=baseline_file, split=\"train\")\n",
    "# baseline = baseline.shuffle(seed=42).add_column(\"topic\", [\"general\"] * len(baseline))\n",
    "\n",
    "# # Nếu baseline quá ít, repeat đến khi đủ 10% khi interleave\n",
    "# target_ratio = 0.05\n",
    "# # math.ceil -- làm tròn lên\n",
    "# baseline_need = math.ceil(len(hotel) * target_ratio / (1 - target_ratio))        # Nhân chéo chia ngang\n",
    "# print(f\"Hotel size: {len(hotel)}, Baseline size: {len(baseline)}, Need baseline size: {baseline_need}\")\n",
    "# step_repeat = max(1, math.ceil(baseline_need / max(1, len(baseline))))  # Tránh chia 0\n",
    "# print(f\"Repeat baseline {step_repeat} times\")\n",
    "# baseline_full = concatenate_datasets([baseline] * step_repeat)\n",
    "# print(f\"New baseline size: {len(baseline_full)}\")\n",
    "\n",
    "# # Interleave tỉ lệ 10% baseline, 90% hotel\n",
    "# mixed = interleave_datasets(\n",
    "#     [hotel, baseline_full],\n",
    "#     probabilities=[0.95, 0.05],\n",
    "#     seed=42,\n",
    "#     stopping_strategy=\"all_exhausted\"\n",
    "# ).shuffle(seed=42)\n",
    "\n",
    "# print(len(mixed), len(hotel) + len(baseline_full))  # phải bằng nhau\n",
    "\n",
    "# # Chia train/val 90/10\n",
    "# mixed_clean = mixed.train_test_split(test_size=0.1, seed=42, shuffle=True)\n",
    "# train_ds = mixed_clean[\"train\"]\n",
    "# val_ds = mixed_clean[\"test\"]\n",
    "\n",
    "# # print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": train_ds,\n",
    "#     \"validation\": val_ds\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062521e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "# from collections import defaultdict\n",
    "# import random, math\n",
    "\n",
    "# hotel_files = [\n",
    "#     \"finetuned-datasets/5-hotel_addresses.json\",\n",
    "#     \"finetuned-datasets/5-hotel_emails.json\",\n",
    "#     \"finetuned-datasets/5-hotel_general.json\",\n",
    "#     \"finetuned-datasets/5-hotel_phones.json\",\n",
    "#     \"finetuned-datasets/5-hotel_roomtypes.json\",\n",
    "# ]\n",
    "# baseline_file = \"finetuned-datasets/5-baseline_general.json\"\n",
    "\n",
    "# SEED = 42\n",
    "# VAL_SEEN_RATIO = 0.15   # 15% mẫu MỖI khách sạn vào validation\n",
    "# BASELINE_RATIO = 0.05   # baseline ~5% tổng số mẫu mỗi split\n",
    "\n",
    "# random.seed(SEED)\n",
    "\n",
    "# # Đảm bảo `ds` có cột `col`\n",
    "# def ensure_col(ds, col, default_val):\n",
    "#     if col not in ds.column_names:\n",
    "#         return ds.add_column(col, [default_val]*len(ds))\n",
    "#     return ds\n",
    "\n",
    "# # Lấy ngẫu nhiên n mẫu từ ds, nếu len(ds) < n thì lấy lặp lại\n",
    "# def up_or_down_sample(ds, n, seed): \n",
    "#     if n <= 0: return ds.select([])\n",
    "#     idx = list(range(len(ds)))\n",
    "#     rng = random.Random(seed)\n",
    "#     if len(ds) >= n:\n",
    "#         rng.shuffle(idx)\n",
    "#         return ds.select(idx[:n])\n",
    "#     picks = [rng.choice(idx) for _ in range(n)]\n",
    "#     return ds.select(picks)\n",
    "\n",
    "# # Trộn baseline vào hotel_split theo target-ratio\n",
    "# def attach_baseline(hotel_split, baseline_ds, ratio, seed):\n",
    "#     h = len(hotel_split)\n",
    "#     if h == 0: return hotel_split\n",
    "#     # ratio ≈ need / (h + need) => need = ratio * (h + need) => need = ratio * h / (1 - ratio)\n",
    "#     need = round(h * (ratio / max(1e-9, 1.0 - ratio)))  # h + need ≈ mục tiêu\n",
    "#     base_pick = up_or_down_sample(baseline_ds, need, seed)\n",
    "#     mixed = concatenate_datasets([hotel_split, base_pick])\n",
    "#     return mixed.shuffle(seed=seed)\n",
    "\n",
    "# # 1) Load\n",
    "# hotel_sets = [load_dataset(\"json\", data_files=f, split=\"train\") for f in hotel_files]\n",
    "# hotel = concatenate_datasets(hotel_sets)\n",
    "# hotel = ensure_col(hotel, \"topic\", \"hotel\")\n",
    "\n",
    "# baseline = load_dataset(\"json\", data_files=baseline_file, split=\"train\")\n",
    "# baseline = ensure_col(baseline, \"topic\", \"general\")\n",
    "\n",
    "# print(\"Hotel size:\", len(hotel), \"Baseline size:\", len(baseline))\n",
    "\n",
    "# # 2) Split SEEN-HOTELS: tách theo mẫu trong MỖI khách sạn\n",
    "# key = \"hotel_name\" if \"hotel_name\" in hotel.column_names else (\n",
    "#       \"hotel_id\" if \"hotel_id\" in hotel.column_names else None)\n",
    "\n",
    "# if key is None:\n",
    "#     # Fallback nếu thiếu khóa nhóm (không khuyến nghị)\n",
    "#     hotel_train, hotel_val = hotel.train_test_split(test_size=VAL_SEEN_RATIO, seed=SEED).values()\n",
    "# else:\n",
    "#     groups = defaultdict(list)\n",
    "#     for i, name in enumerate(hotel[key]):\n",
    "#         groups[str(name)].append(i)\n",
    "\n",
    "#     rng = random.Random(SEED)\n",
    "#     train_idx, val_idx = [], []\n",
    "#     for _, idxs in groups.items():\n",
    "#         rng.shuffle(idxs)\n",
    "#         cut = max(1, round(len(idxs) * VAL_SEEN_RATIO))\n",
    "#         val_idx.extend(idxs[:cut])\n",
    "#         train_idx.extend(idxs[cut:])\n",
    "\n",
    "#     hotel_train = hotel.select(train_idx)\n",
    "#     hotel_val   = hotel.select(val_idx)\n",
    "\n",
    "# # 3) Trộn baseline theo tỉ lệ mục tiêu\n",
    "# train = attach_baseline(hotel_train, baseline, BASELINE_RATIO, seed=SEED)\n",
    "# val   = attach_baseline(hotel_val,   baseline, BASELINE_RATIO, seed=SEED+1)\n",
    "\n",
    "# dataset = DatasetDict({\"train\": train, \"validation\": val})\n",
    "# print(dataset)\n",
    "# print(\"hotel_train:\", len(hotel_train), \"hotel_val:\", len(hotel_val),\n",
    "#       \"train_total:\", len(train), \"val_total:\", len(val))\n",
    "# # ds.save_to_disk(\"finetuned-datasets/trainval-seen-hotels\")\n",
    "# # train.to_json(\"finetuned-datasets/train_seen.jsonl\", lines=True, force_ascii=False)\n",
    "# # val.to_json(\"finetuned-datasets/val_seen.jsonl\", lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faac07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def baseline_freq_from_mixed(mixed_ds, key_cols=(\"question\",\"answer\"), baseline_topic=\"general\", print_top=100):\n",
    "#     if \"topic\" not in mixed_ds.column_names:\n",
    "#         raise ValueError(\"mixed_ds không có cột 'topic' để lọc baseline.\")\n",
    "\n",
    "#     # Lọc phần baseline trong mixed\n",
    "#     mask = [t == baseline_topic for t in mixed_ds[\"topic\"]]\n",
    "#     baseline_rows = mixed_ds.select([i for i, ok in enumerate(mask) if ok])\n",
    "\n",
    "#     # Tạo khóa đếm theo nội dung (tùy schema, chỉnh key_cols cho phù hợp)\n",
    "#     def make_key(i):\n",
    "#         return tuple( (baseline_rows[c][i] if c in baseline_rows.column_names else None) for c in key_cols )\n",
    "\n",
    "#     keys = [make_key(i) for i in range(len(baseline_rows))]\n",
    "#     freq = Counter(keys)\n",
    "\n",
    "#     # Thống kê\n",
    "#     total = len(baseline_rows)\n",
    "#     unique = len(freq)\n",
    "#     num_dups = sum(1 for c in freq.values() if c > 1)\n",
    "#     print(f\"[freq_mixed] baseline_rows={total}, unique_by_key={unique}, duplicated_keys(>1x)={num_dups}\")\n",
    "\n",
    "#     # In top\n",
    "#     for k, c in sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))[:print_top]:\n",
    "#         preview = \" | \".join([str(x)[:80].replace(\"\\n\",\" \") if x is not None else \"None\" for x in k])\n",
    "#         print(f\"  - {c}x | {preview}\")\n",
    "\n",
    "#     return freq\n",
    "\n",
    "# # Sau khi bạn đã có 'train' hoặc 'val' (đối tượng mixed)\n",
    "# freq_from_mixed_train = baseline_freq_from_mixed(train, key_cols=(\"question\",\"answer\"))\n",
    "# freq_from_mixed_val   = baseline_freq_from_mixed(val,   key_cols=(\"question\",\"answer\"))\n",
    "\n",
    "\n",
    "# print(\"freq_from_mixed_train:\", len(freq_from_mixed_train))\n",
    "# print(\"freq_from_mixed_val:\", len(freq_from_mixed_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637fb5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/tyanfarm/hotels-questions-answers-mixed', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tyanfarm/hotels-questions-answers-mixed')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset_name = \"tyanfarm/hotels-questions-answers-mixed\"\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=dataset_name, repo_type=\"dataset\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffbeb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 703.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.78s/ shards]\n",
      "d:\\Tyan\\Quickom\\Simple-Fine-Tune-Llama\\finetune_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tyan\\.cache\\huggingface\\hub\\datasets--tyanfarm--hotels-questions-answers-mixed. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tyanfarm/hotels-questions-answers-mixed/commit/0ecd78dfff4b61ed0fb959348df4899f4c019926', commit_message='Upload dataset', commit_description='', oid='0ecd78dfff4b61ed0fb959348df4899f4c019926', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tyanfarm/hotels-questions-answers-mixed', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tyanfarm/hotels-questions-answers-mixed'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_list(list_conversations)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "})\n",
    "\n",
    "dataset.push_to_hub(dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
